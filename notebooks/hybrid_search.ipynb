{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object main at 0x0000029D7F743A00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import ollama\n",
    "from requests.exceptions import RequestException\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import logging\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import asyncio\n",
    "import faiss\n",
    "import mmap\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Global variables and configuration\n",
    "WEIGHT_SEMANTIC = 0.7  # Initial weight for semantic search\n",
    "MAX_CACHE_SIZE = 1000  # Maximum number of items in LRU cache\n",
    "EMBEDDING_DIMENSION = 384  # Dimension for all-MiniLM-L6-v2 model\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Set up the Ollama client with custom timeout\n",
    "client = ollama.Client(timeout=180)  # 3 minutes timeout\n",
    "\n",
    "# LRU Cache implementation\n",
    "class LRUCache:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.cache = OrderedDict()\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def get(self, key: str) -> Any:\n",
    "        if key not in self.cache:\n",
    "            return None\n",
    "        self.cache.move_to_end(key)\n",
    "        return self.cache[key]\n",
    "\n",
    "    def put(self, key: str, value: Any) -> None:\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "        self.cache[key] = value\n",
    "        if len(self.cache) > self.capacity:\n",
    "            self.cache.popitem(last=False)\n",
    "\n",
    "# Initialize LRU cache\n",
    "lru_cache = LRUCache(MAX_CACHE_SIZE)\n",
    "\n",
    "# Asynchronous function to preload the model\n",
    "async def preload_model(model_name: str) -> None:\n",
    "    try:\n",
    "        await asyncio.to_thread(client.chat, model=model_name, messages=[])\n",
    "        logger.info(f\"Model {model_name} preloaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preloading model {model_name}: {e}\")\n",
    "\n",
    "# Function to load and preprocess documents using multiprocessing\n",
    "def load_documents(file_paths: List[str], max_length: int = 512) -> List[str]:\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        documents = pool.map(partial(process_file, max_length=max_length), file_paths)\n",
    "    documents = [doc for file_docs in documents for doc in file_docs]\n",
    "    logger.info(f\"Total document chunks loaded: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "def process_file(file_path: str, max_length: int) -> List[str]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        chunks = content.split('\\n\\n')\n",
    "        processed_chunks = []\n",
    "        for chunk in chunks:\n",
    "            processed_chunk = preprocess_chunk(chunk)\n",
    "            if processed_chunk:\n",
    "                processed_chunks.extend(split_long_text(processed_chunk, max_length))\n",
    "    logger.info(f\"Loaded document: {file_path}\")\n",
    "    return processed_chunks\n",
    "\n",
    "# Improved text preprocessing function\n",
    "def preprocess_text(text: str) -> List[str]:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "\n",
    "def preprocess_chunk(chunk: str) -> str:\n",
    "    # Remove excessive whitespace and non-printable characters\n",
    "    return re.sub(r'\\s+', ' ', ''.join(char for char in chunk if char.isprintable())).strip()\n",
    "\n",
    "def split_long_text(text: str, max_length: int = 512) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for word in words:\n",
    "        if current_length + len(word) + 1 > max_length:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word)\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += len(word) + 1\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "# Function to generate and cache embeddings using memory-mapped files\n",
    "def generate_and_cache_embeddings(model: SentenceTransformer, documents: List[str], cache_file: str) -> np.ndarray:\n",
    "    if os.path.exists(cache_file):\n",
    "        logger.info(\"Loading cached embeddings...\")\n",
    "        with open(cache_file, 'r+b') as f:\n",
    "            mm = mmap.mmap(f.fileno(), 0)\n",
    "            embeddings = np.frombuffer(mm, dtype=np.float32).reshape(-1, EMBEDDING_DIMENSION)\n",
    "            if len(embeddings) == len(documents):\n",
    "                return embeddings\n",
    "            logger.info(\"Cached embeddings do not match current documents. Regenerating...\")\n",
    "\n",
    "    logger.info(\"Generating embeddings...\")\n",
    "    embeddings = model.encode(documents, convert_to_tensor=True, show_progress_bar=True)\n",
    "    embeddings_np = embeddings.cpu().numpy()\n",
    "\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        f.write(embeddings_np.tobytes())\n",
    "\n",
    "    return embeddings_np\n",
    "\n",
    "# Implement Approximate Nearest Neighbor search using FAISS\n",
    "def build_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatIP:\n",
    "    index = faiss.IndexFlatIP(EMBEDDING_DIMENSION)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# Updated hybrid_search function with FAISS\n",
    "def hybrid_search(query: str, model: SentenceTransformer, documents: List[str], faiss_index: faiss.IndexFlatIP, bm25: BM25Okapi, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Semantic search with FAISS\n",
    "    D, I = faiss_index.search(query_embedding.unsqueeze(0).cpu().numpy(), top_k * 2)\n",
    "    semantic_scores = torch.tensor(D[0])\n",
    "    semantic_indices = torch.tensor(I[0])\n",
    "    \n",
    "    # BM25 search\n",
    "    tokenized_query = preprocess_text(query)\n",
    "    bm25_scores = torch.tensor(bm25.get_scores(tokenized_query))\n",
    "    \n",
    "    # Combine results\n",
    "    bm25_scores_rescored = bm25_scores[semantic_indices]\n",
    "    \n",
    "    # Normalize scores\n",
    "    semantic_scores = (semantic_scores - semantic_scores.min()) / (semantic_scores.max() - semantic_scores.min() + 1e-8)\n",
    "    bm25_scores_rescored = (bm25_scores_rescored - bm25_scores_rescored.min()) / (bm25_scores_rescored.max() - bm25_scores_rescored.min() + 1e-8)\n",
    "    \n",
    "    # Automatic balancing: use the harmonic mean of semantic and lexical scores\n",
    "    combined_scores = 2 / (1/semantic_scores + 1/bm25_scores_rescored)\n",
    "    \n",
    "    top_indices = semantic_indices[torch.argsort(combined_scores, descending=True)][:top_k]\n",
    "    top_scores = combined_scores[torch.argsort(combined_scores, descending=True)][:top_k]\n",
    "    \n",
    "    return [(idx.item(), score.item()) for idx, score in zip(top_indices, top_scores)]\n",
    "\n",
    "def adjust_weight(feedback: str) -> None:\n",
    "    global WEIGHT_SEMANTIC\n",
    "    if feedback == 'more_semantic':\n",
    "        WEIGHT_SEMANTIC = min(1.0, WEIGHT_SEMANTIC + 0.1)\n",
    "    elif feedback == 'more_lexical':\n",
    "        WEIGHT_SEMANTIC = max(0.0, WEIGHT_SEMANTIC - 0.1)\n",
    "    logger.info(f\"Adjusted weight_semantic to {WEIGHT_SEMANTIC}\")\n",
    "\n",
    "# Asynchronous function to interact with Ollama\n",
    "async def ollama_process(query: str, context: str) -> str:\n",
    "    cached_response = lru_cache.get(f\"{query}:{context}\")\n",
    "    if cached_response:\n",
    "        return cached_response\n",
    "\n",
    "    try:\n",
    "        response = await asyncio.to_thread(\n",
    "            client.chat,\n",
    "            model=\"phi3:latest\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant tasked to answer user questions about the company [company]. Use the provided context to answer questions accurately and comprehensively. Provide detailed explanations and examples where appropriate.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {query}\\n\\nPlease provide a detailed and comprehensive answer:\"}\n",
    "            ],\n",
    "            options={\n",
    "                \"num_ctx\": 4096,\n",
    "                \"temperature\": 0.3\n",
    "            }\n",
    "        )\n",
    "        result = response['message']['content']\n",
    "        lru_cache.put(f\"{query}:{context}\", result)\n",
    "        return result\n",
    "    except RequestException as e:\n",
    "        if \"503\" in str(e):\n",
    "            logger.warning(\"The server is currently overloaded. Please try again later.\")\n",
    "            return \"The server is currently overloaded. Please try again later.\"\n",
    "        else:\n",
    "            logger.error(f\"An error occurred: {str(e)}\")\n",
    "            return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def get_user_query() -> str:\n",
    "    return input(\"Enter your question (or 'quit' to exit): \")\n",
    "\n",
    "# Main function to demonstrate hybrid search with Ollama\n",
    "async def main() -> None:\n",
    "    # Preload the model\n",
    "    await preload_model(\"phi3:latest\")\n",
    "    \n",
    "    # Load documents\n",
    "#    file_paths = ['path_to_your_file_1.txt', 'path_to_your_file_2.txt']\n",
    "    documents = load_documents(\"./data\")\n",
    "\n",
    "    if len(documents) < 5:\n",
    "        logger.warning(\"Not enough document chunks for meaningful search. Please check your input files.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Number of documents: {len(documents)}\")\n",
    "\n",
    "    # Initialize the model (using a more suitable pre-trained model for asymmetric search)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generate and cache embeddings\n",
    "    cache_file = 'document_embeddings.mmap'\n",
    "    document_embeddings = generate_and_cache_embeddings(model, documents, cache_file)\n",
    "    \n",
    "    # Build FAISS index\n",
    "    faiss_index = build_faiss_index(document_embeddings)\n",
    "    \n",
    "    # Prepare BM25\n",
    "    tokenized_corpus = [preprocess_text(doc) for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    logger.info(\"Hybrid search system is ready. You can start asking questions.\")\n",
    "    \n",
    "    while True:\n",
    "        query = get_user_query()\n",
    "        if query.lower() == 'quit':\n",
    "            logger.info(\"Thank you for using the [company] hybrid search system. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"Performing hybrid search...\")\n",
    "            results = hybrid_search(query, model, documents, faiss_index, bm25)\n",
    "        except RuntimeError as e:\n",
    "            logger.error(f\"An error occurred during search: {e}\")\n",
    "            results = []\n",
    "        \n",
    "        if not results:\n",
    "            logger.warning(\"No relevant results found. Please try a different query.\")\n",
    "            continue\n",
    "\n",
    "        logger.info(\"\\nTop search results:\")\n",
    "        for idx, (doc_idx, score) in enumerate(results, 1):\n",
    "            logger.info(f\"Result {idx}:\")\n",
    "            logger.info(f\"  Document chunk {doc_idx + 1}\")\n",
    "            logger.info(f\"  Relevance Score: {score:.4f}\")\n",
    "            logger.info(f\"  Content: {documents[doc_idx][:200]}...\")  # Print first 200 characters of the chunk\n",
    "            logger.info(\"\")\n",
    "\n",
    "        # Use Ollama to process the top results\n",
    "        top_context = \"\\n\".join([documents[idx] for idx, _ in results[:3]])\n",
    "        logger.info(\"Generating comprehensive answer using Ollama...\")\n",
    "        ollama_response = await ollama_process(query, top_context)\n",
    "        \n",
    "        logger.info(\"\\nComprehensive Answer:\")\n",
    "        logger.info(ollama_response)\n",
    "        logger.info(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
